{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейный SVM \"своими руками\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерируем обучающую и тестовую выборку для экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_classification(\n",
    "    n_samples=10000, n_features=20, \n",
    "    n_classes=2, n_informative=20, \n",
    "    n_redundant=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(len(X), len(y))\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пишем свой класс для SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Некоторые вычисления для SVM\n",
    "Пусть $x\\in\\mathbb R^n$, $y\\in \\{0, 1\\}$\n",
    "\n",
    "Параметры модели: $w\\in\\mathbb R^n$, $w_0\\in\\mathbb R$\n",
    "\n",
    "Отступ: $M(x, y, w, w_0) = (2y - 1)((w, x) + w_0)$ (поскольку метки $0$, $1$ надо преобразовать к $-1$, $1$)\n",
    "\n",
    "$\\dfrac{dM}{dw} = (2y-1)x,\\quad \\dfrac{dM}{dw_0} = (2y-1)$\n",
    "\n",
    "Ошибка: $L(x, y, w, w_0) = [1 - M(x, y, w, w_0)]_+ = \\left\\{\n",
    "\\begin{aligned}\n",
    "& 1-M,\\quad M\\le1\\\\\n",
    "& 0,\\quad M > 1\n",
    "\\end{aligned}\n",
    "\\right.$\n",
    "\n",
    "$\\dfrac{dL}{dM} = \\left\\{\n",
    "\\begin{aligned}\n",
    "& -1,&M < 1\\\\\n",
    "& 0,&M > 1\n",
    "\\end{aligned}\n",
    "\\right.$\n",
    "\n",
    "$\\dfrac{dL}{dw} = \\left\\{\n",
    "\\begin{aligned}\n",
    "& -(2y-1)x, & M < 1\\\\\n",
    "& 0, & M > 1\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\qquad\n",
    "\\dfrac{dL}{dw_0}= \\left\\{\n",
    "\\begin{aligned}\n",
    "& -(2y-1),& M < 1\\\\\n",
    "& 0,& M > 1\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "$L_2$-регуляризация\n",
    "\n",
    "$R(w) = \\dfrac{||w||^2}{2C}$\n",
    "\n",
    "$\\dfrac{dR}{dw} = \\dfrac{w}{C},\\quad \\dfrac{dR}{dw_0} = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import random\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "class MySVM(object):\n",
    "    def __init__(self, C=10000):\n",
    "        self.C = C # regularization constant\n",
    "\n",
    "    # f(x) = <w,x> + w_0\n",
    "    def f(self, x):\n",
    "        return np.dot(self.w, x) + self.w0\n",
    "\n",
    "    # a(x) = [f(x) > 0]\n",
    "    def a(self, x):\n",
    "        return 1 if self.f(x) > 0 else 0\n",
    "    \n",
    "    # predicting answers for X_test\n",
    "    def predict(self, X_test):\n",
    "        return np.array([self.a(x) for x in X_test])\n",
    "\n",
    "    # l2-regularizator\n",
    "    def reg(self):\n",
    "        return 1.0 * sum(self.w ** 2) / (2.0 * self.C)\n",
    "\n",
    "    # l2-regularizator derivative\n",
    "    # returns tuple (dR/dw, dR/dw0)\n",
    "    def der_reg(self):\n",
    "        return (self.w / self.C, 0.)\n",
    "    \n",
    "    def _margin(self, x, y):\n",
    "        return (2*y - 1) * self.f(x)\n",
    "    \n",
    "    # hinge loss\n",
    "    def loss(self, x, y):\n",
    "        return max([0, 1 - _margin(x, y)])\n",
    "\n",
    "    # hinge loss derivative\n",
    "    # returns tuple (dL/dw, dL/dw0)\n",
    "    def der_loss(self, x, y):\n",
    "        M = self._margin(x, y)\n",
    "        if M > 1:\n",
    "            return (np.zeros_like(self.w), 0.)\n",
    "        return (-(2*y - 1) * x, -(2*y - 1))\n",
    "\n",
    "    # fitting w and w_0 with SGD\n",
    "    def fit(self, X_train, y_train):\n",
    "        dim = len(X_train[0])\n",
    "        self.w = np.random.rand(dim) # initial value for w\n",
    "        self.w0 = np.random.randn() # initial value for w_0\n",
    "        \n",
    "        # 10000 steps is OK for this example\n",
    "        # another variant is to continue iterations while error is still decreasing\n",
    "        for k in range(1, 10000):  \n",
    "            \n",
    "            # random example choise\n",
    "            rand_index = randint(0, len(X_train) - 1) # generating random index\n",
    "            x = X_train[rand_index]\n",
    "            y = y_train[rand_index]\n",
    "\n",
    "            # simple heuristic for step size\n",
    "            # step = 0.5 * 0.9 ** k\n",
    "            # but it is bad so let's try other heuristic\n",
    "            step = 0.1/(k ** 0.5)\n",
    "            \n",
    "            der_L = self.der_loss(x, y)\n",
    "            der_R = self.der_reg()\n",
    "            \n",
    "            der = tuple(der_L[i] + der_R[i] for i in (0, 1))\n",
    "            \n",
    "            # w update\n",
    "            self.w -= step * der[0]\n",
    "            \n",
    "            # w_0 update\n",
    "            self.w0 -= step * der[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пробуем обучить наш классификатор и посмотреть на качество на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.103633   -0.10814459 -0.02799679  0.18783359  0.12175499  0.24175446\n",
      "  0.02568957  0.09778814 -0.14177277 -0.15817862  0.14030208  0.08100593\n",
      "  0.24029485  0.10265193 -0.25146807  0.03528905  0.14307263  0.15529781\n",
      " -0.03183528  0.18684358] -0.13400122342716292\n"
     ]
    }
   ],
   "source": [
    "model = MySVM()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.w, model.w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 0 1] 2000 991\n"
     ]
    }
   ],
   "source": [
    "print(y_test, len(y_test), sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 1012\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions), sum(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7985\n"
     ]
    }
   ],
   "source": [
    "print(sum(predictions == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Задания:\n",
    "\n",
    "### - Допишите недостающие функции в MySVM (производные и обновление весов)\n",
    "\n",
    "### - Сравните качество с sklearn LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При каждом запуске обучения получаются разные результаты, давайте обучим каждую модель несколько раз, чтобы посмотреть на качество в среднем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_accuracy = [0.7995, 0.798, 0.7915, 0.7945, 0.799, 0.8005, 0.798, 0.801, 0.789, 0.791]\n",
      "average = 0.7962\n",
      "CPU times: user 2.96 s, sys: 24 ms, total: 2.98 s\n",
      "Wall time: 2.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "my_model = MySVM(C=10000)\n",
    "my_accuracy = []\n",
    "for i in range(10):\n",
    "    my_model.fit(X_train, y_train)\n",
    "    my_predictions = my_model.predict(X_test)\n",
    "    my_accuracy.append(sum(my_predictions == y_test) / float(len(y_test)))\n",
    "print(\"my_accuracy =\", my_accuracy)\n",
    "print(\"average =\", sum(my_accuracy)/len(my_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skl_accuracy = [0.7965, 0.7965, 0.7965, 0.7965, 0.7965, 0.7965, 0.7965, 0.7965, 0.7965, 0.7965]\n",
      "average = 0.7965\n",
      "CPU times: user 712 ms, sys: 956 ms, total: 1.67 s\n",
      "Wall time: 462 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "skl_model = LinearSVC(C=10000, max_iter=10000, dual=False)\n",
    "skl_accuracy = []\n",
    "for i in range(10):\n",
    "    skl_model.fit(X_train, y_train)\n",
    "    skl_predictions = skl_model.predict(X_test)\n",
    "    skl_accuracy.append(sum(skl_predictions == y_test) / float(len(y_test)))\n",
    "print(\"skl_accuracy =\", skl_accuracy)\n",
    "print(\"average =\", sum(skl_accuracy)/len(skl_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "Наша самодельная модель даёт сравнимое с моделью из sklearn качество, хотя в скорости обучения уступает.\n",
    "\n",
    "Очевидно, что предложенный шаг $h_k = 0.5 \\times 0.9^k$ является плохим выбором, поскольку начиная с некоторого $k$ шаг становится очень близок к нулю, и модель перестаёт обучаться. Например, уже при $k = 100$ шаг $h_k$ имеет порядок $10^{-5}$ и продолжает уменьшаться экспоненциально.\n",
    "\n",
    "Были попробованы некоторые шаги вида $h_k \\sim \\dfrac{1}{k^\\alpha}$, $\\alpha \\in (0, 1]$, и наиболее хорошо себя показал шаг $h_k = \\dfrac{0.1}{\\sqrt{k}}$ -- он дал качество такое же, как у sklearn, остальные были хуже."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
